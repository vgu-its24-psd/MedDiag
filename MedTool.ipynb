{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgu-its24-psd/MedDiag/blob/main/MedTool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMRAG - LLM - START"
      ],
      "metadata": {
        "id": "P71Fikw4lgEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzNopSyJ-hAK"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install -qU langchain-qdrant\n",
        "!pip install -qU langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wlKxfjvAtoh"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from transformers import pipeline # to create the pipeline to LLM Model in Hugging Faces\n",
        "# Qdrant from langchain\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "# HuggingFace embedding model for Qdrant\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QdrantURL = \"\"\n",
        "QdrantAPIKey= \"\""
      ],
      "metadata": {
        "id": "mwJ4V6TFwSvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze_LY33OEwIa"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dengue Fever Clinical Diagnostic System using MedGemma\n",
        "This system analyzes patient symptoms and medical images to assess dengue fever likelihood\n",
        "\"\"\"\n",
        "class DengueDiagnosticSystem:\n",
        "    def __init__(self):\n",
        "      # Medgemma pipeline\n",
        "      #self.diagnostic_pipeline = pipeline(\"image-text-to-text\", model=\"google/medgemma-4b-it\", torch_dtype=torch.bfloat16, device=\"cuda\")\n",
        "      # self.diagnostic_pipeline = pipeline(\"image-text-to-text\", model=\"longbao128/gemma-4b-dengue-diagnosis\", torch_dtype=torch.bfloat16, device=\"cuda\")\n",
        "      self.diagnostic_pipeline = pipeline(\"image-text-to-text\", model=\"google/gemma-3-4b-it\", torch_dtype=torch.bfloat16, device=\"cuda\")\n",
        "\n",
        "      # Qdrant client\n",
        "      client = QdrantClient(url=QdrantURL, api_key=QdrantAPIKey)\n",
        "      embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "      # The retriever (empty to start)\n",
        "      self.RAG = MultiVectorRetriever(\n",
        "                      vectorstore=QdrantVectorStore(client=client, collection_name=\"demo_collection\", embedding=embeddings),\n",
        "                      docstore=InMemoryStore(),\n",
        "                      id_key=\"doc_id\",\n",
        "                      )\n",
        "    def rag_retriever(self, user_input, topk = 1):\n",
        "        docs = self.RAG.vectorstore.similarity_search(user_input, k=topk)\n",
        "        return docs\n",
        "\n",
        "    def generate_diagnosis(self, user_input, list_image_path, docs):\n",
        "        \"\"\"Generate diagnostic assessment using MedGemma\"\"\"\n",
        "         # concatenate page_content\n",
        "        context = \"\\n\".join(d.page_content for d in docs)\n",
        "        system_instruction = \"You are an expert clinical diagnostic AI assistant specializing in infectious diseases.\"\n",
        "        role_instruction = f\"\"\"\n",
        "        User input: {user_input}\n",
        "        Retrieved context: {context}\n",
        "\n",
        "        You are a medical analysis model specialized in dengue fever.\n",
        "\n",
        "        TASK:\n",
        "        Analyze the user input (travel history, age, gender, infection history, reported symptoms) together with the retrieved context (medical guidelines, clinical reports, epidemiological data, and any provided images). Determine the likelihood that the user is infected with dengue fever.\n",
        "\n",
        "        ASSESSMENT RULES:\n",
        "        - Consider key dengue symptoms, risk factors, demographics, and infection history.\n",
        "        - Cross-check user details against contextual evidence and images if provided.\n",
        "        - Decide based on alignment with clinical and epidemiological patterns.\n",
        "\n",
        "        OUTPUT FORMAT:\n",
        "        Respond with one sentence only, using exactly one of the two options:\n",
        "        - \"The user might be infected with dengue fever.\"\n",
        "        - \"The user might not be infected with dengue fever.\"\n",
        "\n",
        "        Do not provide explanations, reasoning, or instructions.\n",
        "        \"\"\"\n",
        "\n",
        "        messages =  self.build_messages(system_instruction, role_instruction, list_image_path)\n",
        "\n",
        "        response = self.diagnostic_pipeline(text=messages, max_new_tokens=200)\n",
        "        #response = self.diagnostic_pipeline(text=messages, max_new_tokens=128, return_full_text=False)\n",
        "            # Extract the text content from the output\n",
        "        return response[0][\"generated_text\"][-1][\"content\"]\n",
        "        #return response[0][\"generated_text\"]\n",
        "\n",
        "    def build_messages(self, system_instruction: str, role_instruction: str, images: list[str] = []):\n",
        "        \"\"\"\n",
        "        images: list of image paths or base64. Can be empty.\n",
        "        \"\"\"\n",
        "        user_content = [{\"type\": \"text\", \"text\": role_instruction}]\n",
        "        for i in images:\n",
        "            im = Image.open(i)\n",
        "            user_content.append({\"type\": \"image\", \"image\": im})\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": system_instruction}],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_content,\n",
        "            },\n",
        "        ]\n",
        "        return messages\n",
        "\n",
        "    def process_case(self, user_input, list_image_path=[]):\n",
        "        \"\"\"Process a complete diagnostic case\"\"\"\n",
        "        print(\"Processing diagnostic case...\")\n",
        "        docs = self.rag_retriever(user_input, topk=3)\n",
        "        # Generate diagnosis\n",
        "        diagnosis = self.generate_diagnosis(user_input, list_image_path, docs)\n",
        "\n",
        "        return diagnosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVcL24T0YFAh"
      },
      "outputs": [],
      "source": [
        "# Initialize Diagnostic System\n",
        "diagnostic_system = DengueDiagnosticSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pEfzQihYJYm"
      },
      "outputs": [],
      "source": [
        "# Interactive mode for Google Colab\n",
        "def interactive_diagnosis():\n",
        "    \"\"\"Interactive mode for easier use in Colab\"\"\"\n",
        "    print(\"Dengue Fever Diagnostic System - Interactive Mode\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Get user input\n",
        "    symptoms = input(\"\\nEnter patient symptoms: \")\n",
        "\n",
        "    image_path = input(\"Enter image path (or press Enter to skip): \").strip()\n",
        "    if not image_path:\n",
        "        image_paths = [] # Pass an empty list if no image is provided\n",
        "    elif not os.path.exists(image_path):\n",
        "        print(f\"Warning: Image file '{image_path}' not found. Proceeding without image.\")\n",
        "        image_paths = [] # Pass an empty list if image file not found\n",
        "    else:\n",
        "        image_paths = [image_path] # Pass a list with the image path\n",
        "\n",
        "    # Run diagnosis\n",
        "    result = diagnostic_system.process_case(symptoms, image_paths)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DENGUE FEVER DIAGNOSTIC ASSESSMENT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Symptoms: {symptoms}\")\n",
        "    if image_paths:\n",
        "        print(f\"Image: {image_paths[0]}\") # Display the first image path if available\n",
        "    print(\"\\n CLINICAL ANALYSIS:\")\n",
        "    print(result)\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuAriKJZYMBY"
      },
      "outputs": [],
      "source": [
        "# Example usage in Colab:\n",
        "\"\"\"\n",
        "# Example symptoms to test:\n",
        "# \"sudden high fever 39Â°C, severe headache, retro-orbital pain, myalgia, skin rash, nausea\"\n",
        "# \"fever, headache, muscle pain, petechiae, bleeding gums, abdominal pain\"\n",
        "# \"mild fever, cough, runny nose\" (should be low likelihood)\n",
        "\"\"\"\n",
        "\n",
        "interactive_diagnosis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEgtzP0_jekX"
      },
      "outputs": [],
      "source": [
        "# test rag_retriever\n",
        "query = \"I have fever and headache\"\n",
        "diagnostic_system.rag_retriever(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8uN_48FZBuv"
      },
      "outputs": [],
      "source": [
        "# test build_messages()\n",
        "image = [\"/content/Skin rash from dengue fever_p1_img23_434c0412.png\"]\n",
        "diagnostic_system.build_messages(\"My system instruction\", \"my role instruction\", image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMRAG - LLM - END"
      ],
      "metadata": {
        "id": "cPYF7lhylloH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Added interface to GUI - START"
      ],
      "metadata": {
        "id": "G9nge5q0lVH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-cors pyngrok"
      ],
      "metadata": {
        "id": "T8lFPyVzlWAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "J-GNnMYylpKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrokTOKEN = \"\""
      ],
      "metadata": {
        "id": "GB4AmX8twtfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assume diagnostic_system = DengueDiagnosticSystem(...)\n",
        "# already initialized\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/diagnose\", methods=[\"POST\"])\n",
        "def diagnose():\n",
        "    data = request.get_json()\n",
        "    user_input = data.get(\"user_input\", \"\")\n",
        "    images_b64 = data.get(\"images\", [])\n",
        "\n",
        "    list_image_path = []\n",
        "    for img_b64 in images_b64:\n",
        "        # save temporary file in Colab\n",
        "        fd, tmp_path = tempfile.mkstemp(suffix=\".png\")\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            f.write(base64.b64decode(img_b64))\n",
        "        list_image_path.append(tmp_path)\n",
        "\n",
        "    result = diagnostic_system.process_case(user_input, list_image_path)\n",
        "    return jsonify({\"result\": result})\n",
        "\n",
        "# Set your ngrok authtoken here\n",
        "# Replace \"YOUR_NGROK_AUTHTOKEN\" with your actual authtoken\n",
        "ngrok.set_auth_token(ngrokTOKEN)\n",
        "\n",
        "# tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "WMCwOJ8llrD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Added interface to GUI - END"
      ],
      "metadata": {
        "id": "uFKmwkC3lWRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation  - START"
      ],
      "metadata": {
        "id": "5D0kd_wMlYMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPUDiMw529b3"
      },
      "outputs": [],
      "source": [
        "!pip install -qU ragas\n",
        "!pip install -qU pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JgjwgVt3CB6"
      },
      "outputs": [],
      "source": [
        "# Import RAGAS and evaluation dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        ")\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y5M3jy-3B3n"
      },
      "outputs": [],
      "source": [
        "# Load and prepare evaluation dataset\n",
        "eval_df = pd.read_csv('eval_dengue.csv')\n",
        "print(\"Evaluation dataset shape:\", eval_df.shape)\n",
        "print(\"\\nDataset columns:\", eval_df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(eval_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf2SgWb-3Uko"
      },
      "outputs": [],
      "source": [
        "# Prepare evaluation data from dengue CSV\n",
        "def prepare_evaluation_data(eval_df, sample_size=10):\n",
        "    \"\"\"Convert dengue patient data to evaluation format\"\"\"\n",
        "    evaluation_data = []\n",
        "\n",
        "    for idx, row in eval_df.head(sample_size).iterrows():\n",
        "        # Create symptom description from patient data\n",
        "        symptoms = []\n",
        "        if pd.notna(row.get('dengue.current_temp', 0)) and row.get('dengue.current_temp', 0) > 0:\n",
        "            symptoms.append(f\"fever {row['dengue.current_temp']}Â°F\")\n",
        "        if row.get('dengue.servere_headche') == 'yes':\n",
        "            symptoms.append(\"severe headache\")\n",
        "        if row.get('dengue.pain_behind_the_eyes') == 'yes':\n",
        "            symptoms.append(\"retro-orbital pain\")\n",
        "        if row.get('dengue.joint_muscle_aches') == 'yes':\n",
        "            symptoms.append(\"muscle and joint aches\")\n",
        "        if row.get('dengue.metallic_taste_in_the_mouth') == 'yes':\n",
        "            symptoms.append(\"metallic taste\")\n",
        "        if row.get('dengue.appetite_loss') == 'yes':\n",
        "            symptoms.append(\"loss of appetite\")\n",
        "        if row.get('dengue.addominal_pain') == 'yes':\n",
        "            symptoms.append(\"abdominal pain\")\n",
        "        if row.get('dengue.nausea_vomiting') == 'yes':\n",
        "            symptoms.append(\"nausea and vomiting\")\n",
        "        if row.get('dengue.diarrhoea') == 'yes':\n",
        "            symptoms.append(\"diarrhea\")\n",
        "\n",
        "        # Create patient query\n",
        "        travel_history = f\"recently traveled to {row['dengue.residence']}\" if pd.notna(row['dengue.residence']) else \"\"\n",
        "        duration = f\"symptoms for {row['dengue.days']}\" if pd.notna(row['dengue.days']) else \"\"\n",
        "\n",
        "        question = f\"Patient with {', '.join(symptoms)}. {travel_history}. {duration}. Could this be dengue fever?\"\n",
        "\n",
        "        # Ground truth answer\n",
        "        ground_truth = \"The user might be infected with dengue fever.\" if row['dengue.dengue'] == 'yes' else \"The user might not be infected with dengue fever.\"\n",
        "\n",
        "        evaluation_data.append({\n",
        "            'patient_id': row['dengue.p_i_d'],\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'actual_diagnosis': row['dengue.dengue']\n",
        "        })\n",
        "\n",
        "    return evaluation_data\n",
        "\n",
        "# Prepare evaluation dataset\n",
        "eval_data = prepare_evaluation_data(eval_df, sample_size=10)\n",
        "print(f\"Prepared {len(eval_data)} evaluation cases\")\n",
        "for i, case in enumerate(eval_data[:3]):\n",
        "    print(f\"\\nCase {i+1}:\")\n",
        "    print(f\"Question: {case['question']}\")\n",
        "    print(f\"Ground Truth: {case['ground_truth']}\")\n",
        "    print(f\"Actual: {case['actual_diagnosis']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1x3ZW0n3dqy"
      },
      "outputs": [],
      "source": [
        "# RAG System Evaluation with RAGAS\n",
        "def evaluate_diagnostic_system(diagnostic_system, eval_data):\n",
        "    \"\"\"Evaluate the diagnostic system using RAGAS metrics\"\"\"\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    contexts = []\n",
        "    ground_truths = []\n",
        "\n",
        "    print(\"Running diagnostic system on evaluation cases...\")\n",
        "\n",
        "    for i, case in enumerate(eval_data):\n",
        "        try:\n",
        "            print(f\"Processing case {i+1}/{len(eval_data)}: {case['patient_id']}\")\n",
        "\n",
        "            # Get RAG response\n",
        "            query = case['question']\n",
        "\n",
        "            # Retrieve context documents\n",
        "            docs = diagnostic_system.rag_retriever(query, topk=3)\n",
        "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            # Generate diagnosis\n",
        "            diagnosis = diagnostic_system.generate_diagnosis(query, [], docs)\n",
        "\n",
        "            # Store results for RAGAS evaluation\n",
        "            questions.append(query)\n",
        "            answers.append(diagnosis)\n",
        "            contexts.append([context_text])  # RAGAS expects list of contexts\n",
        "            ground_truths.append(case['ground_truth'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing case {case['patient_id']}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Create RAGAS dataset\n",
        "    ragas_dataset = Dataset.from_dict({\n",
        "        \"question\": questions,\n",
        "        \"answer\": answers,\n",
        "        \"contexts\": contexts,\n",
        "        \"ground_truth\": ground_truths\n",
        "    })\n",
        "\n",
        "    return ragas_dataset\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Starting RAGAS evaluation...\")\n",
        "ragas_dataset = evaluate_diagnostic_system(diagnostic_system, eval_data)\n",
        "print(f\"Created dataset with {len(ragas_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5K7_DtE3jrz"
      },
      "outputs": [],
      "source": [
        "def evaluate_diagnostic_system_custom(diagnostic_system, eval_data):\n",
        "    \"\"\"Evaluate using your existing MedGemma + RAG system\"\"\"\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    contexts = []\n",
        "    ground_truths = []\n",
        "\n",
        "    print(\"Running diagnostic system evaluation using MedGemma + RAG...\")\n",
        "\n",
        "    for i, case in enumerate(eval_data):\n",
        "        try:\n",
        "            print(f\"Processing case {i+1}/{len(eval_data)}: {case['patient_id']}\")\n",
        "\n",
        "            # Use your existing process_case method\n",
        "            query = case['question']\n",
        "\n",
        "            # Get diagnosis using your complete system (RAG + MedGemma)\n",
        "            diagnosis = diagnostic_system.process_case(query, [])  # No images for now\n",
        "\n",
        "            # Get context for evaluation (retrieve separately for analysis)\n",
        "            docs = diagnostic_system.rag_retriever(query, topk=3)\n",
        "            context_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            # Store results\n",
        "            questions.append(query)\n",
        "            answers.append(diagnosis)\n",
        "            contexts.append([context_text])\n",
        "            ground_truths.append(case['ground_truth'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing case {case['patient_id']}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Create evaluation dataset\n",
        "    eval_results = {\n",
        "        \"question\": questions,\n",
        "        \"answer\": answers,\n",
        "        \"contexts\": contexts,\n",
        "        \"ground_truth\": ground_truths\n",
        "    }\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "def calculate_custom_metrics(eval_results):\n",
        "    \"\"\"Calculate evaluation metrics without external APIs\"\"\"\n",
        "\n",
        "    print(\"Calculating custom evaluation metrics...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(len(eval_results['question'])):\n",
        "        question = eval_results['question'][i]\n",
        "        answer = eval_results['answer'][i]\n",
        "        ground_truth = eval_results['ground_truth'][i]\n",
        "        context = eval_results['contexts'][i][0] if eval_results['contexts'][i] else \"\"\n",
        "\n",
        "        # 1. Answer Relevancy (keyword overlap with question)\n",
        "        question_words = set(question.lower().split())\n",
        "        answer_words = set(answer.lower().split())\n",
        "        common_words = question_words.intersection(answer_words)\n",
        "        answer_relevancy = len(common_words) / len(question_words) if question_words else 0\n",
        "\n",
        "        # 2. Answer Correctness (check if diagnosis matches)\n",
        "        # Extract diagnosis prediction from answer\n",
        "        answer_lower = answer.lower()\n",
        "        ground_lower = ground_truth.lower()\n",
        "\n",
        "        if \"might be infected\" in ground_lower and \"might be infected\" in answer_lower:\n",
        "            answer_correctness = 1.0\n",
        "        elif \"might not be infected\" in ground_lower and \"might not be infected\" in answer_lower:\n",
        "            answer_correctness = 1.0\n",
        "        elif \"might be infected\" in ground_lower and \"might not\" in answer_lower:\n",
        "            answer_correctness = 0.0\n",
        "        elif \"might not be infected\" in ground_lower and (\"might be\" in answer_lower and \"might not\" not in answer_lower):\n",
        "            answer_correctness = 0.0\n",
        "        else:\n",
        "            # Partial match based on key terms\n",
        "            if \"dengue\" in answer_lower and \"dengue\" in ground_lower:\n",
        "                answer_correctness = 0.5\n",
        "            else:\n",
        "                answer_correctness = 0.0\n",
        "\n",
        "        # 3. Context Precision (how much of context is used in answer)\n",
        "        context_words = set(context.lower().split()) if context else set()\n",
        "        context_used = context_words.intersection(answer_words)\n",
        "        context_precision = len(context_used) / len(context_words) if context_words else 0\n",
        "\n",
        "        # 4. Faithfulness (answer should be based on context, not hallucinated)\n",
        "        # Check if answer contains information not in context\n",
        "        medical_terms = {\"fever\", \"headache\", \"dengue\", \"symptoms\", \"temperature\", \"travel\", \"infection\"}\n",
        "        answer_medical = medical_terms.intersection(answer_words)\n",
        "        context_medical = medical_terms.intersection(context_words) if context_words else set()\n",
        "\n",
        "        if answer_medical and context_medical:\n",
        "            faithfulness = len(answer_medical.intersection(context_medical)) / len(answer_medical)\n",
        "        else:\n",
        "            faithfulness = 0.5  # neutral if no medical terms\n",
        "\n",
        "        # 5. Answer Similarity (semantic similarity to ground truth)\n",
        "        # Simple word overlap similarity\n",
        "        ground_words = set(ground_truth.lower().split())\n",
        "        similarity = len(answer_words.intersection(ground_words)) / len(answer_words.union(ground_words)) if answer_words.union(ground_words) else 0\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'ground_truth': ground_truth,\n",
        "            'contexts': eval_results['contexts'][i],\n",
        "            'answer_relevancy': answer_relevancy,\n",
        "            'answer_correctness': answer_correctness,\n",
        "            'context_precision': context_precision,\n",
        "            'faithfulness': faithfulness,\n",
        "            'answer_similarity': similarity\n",
        "        })\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Create a results object that mimics RAGAS output\n",
        "    class CustomResults:\n",
        "        def __init__(self, df):\n",
        "            self.df = df\n",
        "        def to_pandas(self):\n",
        "            return self.df\n",
        "\n",
        "    return CustomResults(results_df)\n",
        "\n",
        "# Run evaluation using your MedGemma system\n",
        "print(\"Starting evaluation with your MedGemma + RAG system...\")\n",
        "eval_results = evaluate_diagnostic_system_custom(diagnostic_system, eval_data)\n",
        "evaluation_results = calculate_custom_metrics(eval_results)\n",
        "print(f\"Evaluation completed on {len(eval_results['question'])} test cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuIfmz415OvE"
      },
      "outputs": [],
      "source": [
        "# Display Comprehensive Results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RAGAS EVALUATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Convert results to DataFrame for better visualization\n",
        "results_df = evaluation_results.to_pandas()\n",
        "print(f\"\\nEvaluation completed on {len(results_df)} test cases\")\n",
        "print(\"\\nOverall Metrics Summary:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Calculate and display metric means\n",
        "for column in results_df.columns:\n",
        "    if column not in ['question', 'answer', 'contexts', 'ground_truth']:\n",
        "        mean_score = results_df[column].mean()\n",
        "        print(f\"{column.replace('_', ' ').title()}: {mean_score:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Results:\")\n",
        "print(\"-\" * 50)\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Show specific cases with high and low performance\n",
        "if 'answer_relevancy' in results_df.columns:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CASE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Best performing case\n",
        "    best_idx = results_df['answer_relevancy'].idxmax()\n",
        "    print(f\"\\nBest Performing Case (Answer Relevancy: {results_df.loc[best_idx, 'answer_relevancy']:.4f}):\")\n",
        "    print(f\"Question: {results_df.loc[best_idx, 'question']}\")\n",
        "    print(f\"Generated Answer: {results_df.loc[best_idx, 'answer']}\")\n",
        "    print(f\"Ground Truth: {results_df.loc[best_idx, 'ground_truth']}\")\n",
        "\n",
        "    # Worst performing case\n",
        "    worst_idx = results_df['answer_relevancy'].idxmin()\n",
        "    print(f\"\\nWorst Performing Case (Answer Relevancy: {results_df.loc[worst_idx, 'answer_relevancy']:.4f}):\")\n",
        "    print(f\"Question: {results_df.loc[worst_idx, 'question']}\")\n",
        "    print(f\"Generated Answer: {results_df.loc[worst_idx, 'answer']}\")\n",
        "    print(f\"Ground Truth: {results_df.loc[worst_idx, 'ground_truth']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7UoI8Qb5ZWf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze results and provide insights\n",
        "if 'answer_relevancy' in results_df.columns:\n",
        "    relevancy_score = results_df['answer_relevancy'].mean()\n",
        "    print(f\"\\nANSWER RELEVANCY: {relevancy_score:.4f}\")\n",
        "\n",
        "if 'faithfulness' in results_df.columns:\n",
        "    faithfulness_score = results_df['faithfulness'].mean()\n",
        "    print(f\"FAITHFULNESS: {faithfulness_score:.4f}\")\n",
        "\n",
        "if 'context_precision' in results_df.columns:\n",
        "    precision_score = results_df['context_precision'].mean()\n",
        "    print(f\"CONTEXT PRECISION: {precision_score:.4f}\")\n",
        "\n",
        "if 'answer_correctness' in results_df.columns:\n",
        "    correctness_score = results_df['answer_correctness'].mean()\n",
        "    print(f\"\\nANSWER CORRECTNESS: {correctness_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation  - END"
      ],
      "metadata": {
        "id": "lHcnKHOrlcJl"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}