{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1KewsRFngKRCUpruKXIyMQgYvnCyvd0FJ",
      "authorship_tag": "ABX9TyNhExXp2d/Amgk+ZhY1kkXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/long-nguyen-bao-ts/bitnami-helm-charts/blob/main/fine_tune_medgemma_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune MedGemma for Dengue Fever Diagnosis\n",
        "\n",
        "This notebook demonstrates fine-tuning MedGemma on a multimodal dengue fever diagnosis task using clinical data and medical images. The model will learn to classify patients as having dengue fever or not based on clinical symptoms, laboratory values, and associated medical images.\n",
        "\n",
        "The fine-tuning uses:\n",
        "- **Clinical Data**: CSV file containing patient symptoms, lab values, and dengue diagnosis\n",
        "- **Medical Images**: Associated patient images showing potential dengue-related symptoms\n",
        "- **Task**: Binary classification (dengue positive/negative)\n",
        "- **Technique**: QLoRA (Quantized Low-Rank Adaptation) for efficient fine-tuning\n"
      ],
      "metadata": {
        "id": "6GkDpnFu904L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "5nT_2VIZ99w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure HF\n"
      ],
      "metadata": {
        "id": "L_dSHk8p-JUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
        "    # Use secret if running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "3GLoqIj7-P7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "ofBdU54Z-beN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet bitsandbytes datasets evaluate peft tensorboard transformers trl pandas pillow matplotlib"
      ],
      "metadata": {
        "id": "6ljsTjvc-b9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dengue diagnosis dataset\n",
        "\n",
        "This implements **training** for dengue fever diagnosis with MedGemma:\n",
        "\n",
        "### Training Strategy:\n",
        "**Multimodal Training**: Train on both clinical data (CSV) and medical images simultaneously\n",
        "- Single training pipeline that handles both text-only and image+text samples\n",
        "- Dynamic data type detection during training\n",
        "- Mixed batches containing both modalities\n",
        "- Unified evaluation across both data types\n",
        "\n",
        "### Dataset Components:\n",
        "- **Clinical Text Data**: Patient records from CSV with symptoms, lab values, diagnosis\n",
        "- **Medical Images**: Dengue-positive medical images for visual learning\n",
        "- **Unified Approach**: Both modalities trained together in a single step\n"
      ],
      "metadata": {
        "id": "6QqQaRcr0w5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from datasets import Dataset, DatasetDict\n",
        "from typing import Any, List, Dict\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/PDS I/train/train_dengue.csv\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PQTeXHxs031D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nSample image:\")\n",
        "img = Image.open('/content/drive/My Drive/PDS I/train/images/4650d3e0-f00e-4fe9-a057-9b9106d3d976_p1_img23_434c0412.png')\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evJaNXaMnsK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['dengue_binary'] = df['dengue.dengue'].map({'yes': 1, 'no': 0})\n",
        "df = df.dropna(subset=['dengue_binary'])\n",
        "df = df[df['dengue_binary'].notna()]\n",
        "\n",
        "# For symptoms, distinguish between missing data (NaN) and explicit 'no' responses\n",
        "symptom_columns = [\n",
        "    'dengue.servere_headche', 'dengue.pain_behind_the_eyes',\n",
        "    'dengue.joint_muscle_aches', 'dengue.metallic_taste_in_the_mouth',\n",
        "    'dengue.appetite_loss', 'dengue.addominal_pain',\n",
        "    'dengue.nausea_vomiting', 'dengue.diarrhoea'\n",
        "]\n",
        "\n",
        "# For non-symptom columns, fill with 'unknown' but handle symptoms differently\n",
        "non_symptom_columns = [col for col in df.columns if col not in symptom_columns + ['dengue.dengue', 'dengue_binary']]\n",
        "df[non_symptom_columns] = df[non_symptom_columns].fillna('unknown')\n",
        "\n",
        "# For symptom columns, keep NaN to distinguish from explicit 'no'\n",
        "# This helps the model understand when data is truly missing vs confirmed absent\n",
        "\n",
        "print(f\"\\nText Training Dataset:\")\n",
        "print(f\"Total clinical records: {len(df)}\")\n",
        "print(f\"Dengue positive cases: {df['dengue_binary'].sum()}\")\n",
        "print(f\"Dengue negative cases: {(df['dengue_binary'] == 0).sum()}\")\n",
        "\n",
        "\n",
        "image_base_path = \"/content/drive/My Drive/PDS I/train/images\"\n",
        "image_files = glob.glob(f\"{image_base_path}/*.png\") + glob.glob(f\"{image_base_path}/*.jpg\") + glob.glob(f\"{image_base_path}/*.jpeg\")\n",
        "print(f\"\\nImage Training Dataset:\")\n",
        "print(f\"Total medical images: {len(image_files)} (all dengue positive)\")"
      ],
      "metadata": {
        "id": "Gat5DSvZ08GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Training Data\n",
        "\n",
        "Create a dataset that combines both text-only and multimodal samples for simultaneous training."
      ],
      "metadata": {
        "id": "VvMM2MKj9S8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clinical_summary(row):\n",
        "    \"\"\"Create a clinical summary from patient data with improved handling of unknown symptoms.\"\"\"\n",
        "    summary_parts = []\n",
        "\n",
        "    # Demographics and basic info\n",
        "    if row['dengue.residence'] != 'unknown':\n",
        "        summary_parts.append(f\"Patient from {row['dengue.residence']}\")\n",
        "\n",
        "    if row['dengue.days'] != 'unknown':\n",
        "        summary_parts.append(f\"fever duration: {row['dengue.days']}\")\n",
        "\n",
        "    if row['dengue.current_temp'] != 'unknown':\n",
        "        summary_parts.append(f\"current temperature: {row['dengue.current_temp']}Â°F\")\n",
        "\n",
        "    # Lab values with proper handling of unknowns\n",
        "    lab_values = []\n",
        "    lab_mapping = {\n",
        "        'dengue.wbc': 'WBC',\n",
        "        'dengue.hemoglobin': 'Hemoglobin',\n",
        "        'dengue._hematocri': 'Hematocrit',\n",
        "        'dengue.platelet': 'Platelet'\n",
        "    }\n",
        "\n",
        "    for col, lab_name in lab_mapping.items():\n",
        "        if row[col] != 'unknown' and pd.notna(row[col]):\n",
        "            lab_values.append(f\"{lab_name}: {row[col]}\")\n",
        "\n",
        "    if lab_values:\n",
        "        summary_parts.append(f\"Lab values - {', '.join(lab_values)}\")\n",
        "\n",
        "    # Improved symptom handling\n",
        "    symptom_mapping = {\n",
        "        'dengue.servere_headche': 'severe headache',\n",
        "        'dengue.pain_behind_the_eyes': 'pain behind eyes',\n",
        "        'dengue.joint_muscle_aches': 'joint/muscle aches',\n",
        "        'dengue.metallic_taste_in_the_mouth': 'metallic taste',\n",
        "        'dengue.appetite_loss': 'appetite loss',\n",
        "        'dengue.addominal_pain': 'abdominal pain',\n",
        "        'dengue.nausea_vomiting': 'nausea/vomiting',\n",
        "        'dengue.diarrhoea': 'diarrhea'\n",
        "    }\n",
        "\n",
        "    present_symptoms = []\n",
        "    absent_symptoms = []\n",
        "    unknown_symptoms = []\n",
        "\n",
        "    for col, symptom_name in symptom_mapping.items():\n",
        "        if pd.isna(row[col]):  # Truly missing data\n",
        "            unknown_symptoms.append(symptom_name)\n",
        "        elif row[col] == 'yes':  # Explicitly present\n",
        "            present_symptoms.append(symptom_name)\n",
        "        elif row[col] == 'no':   # Explicitly absent\n",
        "            absent_symptoms.append(symptom_name)\n",
        "\n",
        "    # Build symptom summary with clinical context\n",
        "    if present_symptoms:\n",
        "        summary_parts.append(f\"Present symptoms: {', '.join(present_symptoms)}\")\n",
        "\n",
        "    if absent_symptoms:\n",
        "        summary_parts.append(f\"Absent symptoms: {', '.join(absent_symptoms)}\")\n",
        "\n",
        "    # Only mention unknown symptoms if there are many, to avoid cluttering\n",
        "    if len(unknown_symptoms) > 0:\n",
        "        if len(unknown_symptoms) <= 3:\n",
        "            summary_parts.append(f\"Symptom status unknown for: {', '.join(unknown_symptoms)}\")\n",
        "        else:\n",
        "            summary_parts.append(f\"Symptom status unknown for {len(unknown_symptoms)} symptoms\")\n",
        "\n",
        "    return '. '.join(summary_parts) + '.'\n",
        "\n",
        "# Test the improved function\n",
        "sample_summary = create_clinical_summary(df.iloc[0])\n",
        "print(f\"Sample clinical summary: {sample_summary}\")\n",
        "sample_summary = create_clinical_summary(df.iloc[1])\n",
        "print(f\"Sample clinical summary: {sample_summary}\")\n",
        "sample_summary = create_clinical_summary(df.iloc[2])\n",
        "print(f\"Sample clinical summary: {sample_summary}\")"
      ],
      "metadata": {
        "id": "pI7cQOqf2LCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_text_only_data(row):\n",
        "    \"\"\"Format clinical text data for training (text-only samples).\"\"\"\n",
        "    clinical_summary = create_clinical_summary(row)\n",
        "\n",
        "    prompt = f\"\"\"Based on the following clinical presentation, please diagnose whether this patient has dengue fever.\n",
        "\n",
        "Clinical Information:\n",
        "{clinical_summary}\n",
        "\n",
        "Based on the clinical data above, does this patient have dengue fever?\n",
        "\n",
        "Respond with either \"Yes - Dengue positive\" or \"No - Dengue negative\".\"\"\"\n",
        "\n",
        "    answer = \"Yes - Dengue positive\" if row['dengue_binary'] == 1 else \"No - Dengue negative\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": answer,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        \"label\": row['dengue_binary'],\n",
        "        \"data_type\": \"text_only\",\n",
        "        \"image\": None\n",
        "    }\n",
        "\n",
        "def format_multimodal_data(image_path):\n",
        "    \"\"\"Format image data for unified training (multimodal samples).\"\"\"\n",
        "\n",
        "    prompt = \"\"\"Analyze this medical image for signs of dengue fever.\n",
        "\n",
        "Based on the visual evidence in this medical image, does this show signs consistent with dengue fever?\n",
        "\n",
        "Respond with \"Yes - Dengue positive\" since this image shows dengue-related symptoms.\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt,\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"Yes - Dengue positive\",\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        \"image\": image_path,\n",
        "        \"label\": 1,\n",
        "        \"data_type\": \"multimodal\"\n",
        "    }\n",
        "\n",
        "\n",
        "sample_text_formatted = format_text_only_data(df.iloc[0])\n",
        "sample_multimodal_formatted = format_multimodal_data(image_files[0])\n",
        "\n",
        "print(\"Format created successfully\")\n",
        "print(f\"Text sample type: {sample_text_formatted['data_type']}\")\n",
        "print(f\"Text sample label: {sample_text_formatted['label']}\")\n",
        "\n",
        "print(f\"\\nMultimodal sample type: {sample_multimodal_formatted['data_type']}\")\n",
        "print(f\"Multimodal sample label: {sample_multimodal_formatted['label']}\")\n",
        "print(f\"Image path: {sample_multimodal_formatted['image']}\")"
      ],
      "metadata": {
        "id": "PQfZ_M0w2RrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== TRAINING DATASET ===\")\n",
        "\n",
        "unified_dataset_list = []\n",
        "\n",
        "print(\"Processing text-only samples from CSV...\")\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        formatted_data = format_text_only_data(row)\n",
        "        unified_dataset_list.append(formatted_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Successfully processed {len(unified_dataset_list)} text-only samples\")\n",
        "\n",
        "print(\"\\nProcessing multimodal samples from images...\")\n",
        "image_samples_added = 0\n",
        "for image_path in image_files:\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        formatted_data = format_multimodal_data(image_path)\n",
        "        formatted_data['image'] = img\n",
        "        unified_dataset_list.append(formatted_data)\n",
        "        image_samples_added += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Successfully processed {image_samples_added} multimodal samples\")\n",
        "\n",
        "unified_dataset = Dataset.from_list(unified_dataset_list)\n",
        "\n",
        "from datasets import ClassLabel\n",
        "unified_dataset = unified_dataset.cast_column(\"label\", ClassLabel(names=[\"No Dengue\", \"Dengue\"]))\n",
        "\n",
        "if len(unified_dataset) <= 5:\n",
        "    print(\"Small dataset detected - using simple split\")\n",
        "    unified_train_test_split = unified_dataset.train_test_split(test_size=0.3, seed=42)\n",
        "else:\n",
        "    unified_train_test_split = unified_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
        "\n",
        "unified_data = DatasetDict({\n",
        "    \"train\": unified_train_test_split[\"train\"],\n",
        "    \"validation\": unified_train_test_split[\"test\"]\n",
        "})\n",
        "\n",
        "\n",
        "train_text_count = sum(1 for sample in unified_data[\"train\"] if sample[\"data_type\"] == \"text_only\")\n",
        "train_multimodal_count = sum(1 for sample in unified_data[\"train\"] if sample[\"data_type\"] == \"multimodal\")\n",
        "val_text_count = sum(1 for sample in unified_data[\"validation\"] if sample[\"data_type\"] == \"text_only\")\n",
        "val_multimodal_count = sum(1 for sample in unified_data[\"validation\"] if sample[\"data_type\"] == \"multimodal\")\n",
        "\n",
        "print(f\"\\nDataset Splits:\")\n",
        "print(f\"  Train: {len(unified_data['train'])} samples\")\n",
        "print(f\"    - Text-only: {train_text_count} samples\")\n",
        "print(f\"    - Multimodal: {train_multimodal_count} samples\")\n",
        "print(f\"  Validation: {len(unified_data['validation'])} samples\")\n",
        "print(f\"    - Text-only: {val_text_count} samples\")\n",
        "print(f\"    - Multimodal: {val_multimodal_count} samples\")\n",
        "\n",
        "\n",
        "train_positive = sum(unified_data['train']['label'])\n",
        "train_negative = len(unified_data['train']) - train_positive\n",
        "val_positive = sum(unified_data['validation']['label'])\n",
        "val_negative = len(unified_data['validation']) - val_positive\n",
        "\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "print(f\"  Train - Positive: {train_positive}, Negative: {train_negative}\")\n",
        "print(f\"  Validation - Positive: {val_positive}, Negative: {val_negative}\")\n",
        "\n",
        "print(f\"\\n=== TRAINING SUMMARY ===\")\n",
        "print(f\"Total samples: {len(unified_dataset_list)}\")\n",
        "print(f\"Text-only samples: {len(unified_dataset_list) - image_samples_added}\")\n",
        "print(f\"Multimodal samples: {image_samples_added}\")"
      ],
      "metadata": {
        "id": "vXjn6_B02T6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning with QLoRA\n",
        "\n",
        "Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is Low-Rank Adaptation (LoRA), which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices. In QLoRA, the base model is quantized to 4-bit before its weights are frozen, then LoRA adapter layers are attached and trained.\n",
        "\n",
        "Fine-tuning MedGemma with QLoRA using the `SFTTrainer` from the Hugging Face `TRL` library."
      ],
      "metadata": {
        "id": "ql6WBrlK9RLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model from Hugging Face Hub\n"
      ],
      "metadata": {
        "id": "XciSyR1o9WBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/gemma-3-4b-it\"\n",
        "\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "processor.tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "G3jaAYjd9Wtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up for fine-tuning\n"
      ],
      "metadata": {
        "id": "mTT-1FHDj_qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "4EWhS-H7j9zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "def unified_collate_fn(examples: list[dict[str, Any]]):\n",
        "\n",
        "    text_only_examples = [ex for ex in examples if ex[\"data_type\"] == \"text_only\"]\n",
        "    multimodal_examples = [ex for ex in examples if ex[\"data_type\"] == \"multimodal\"]\n",
        "\n",
        "    if text_only_examples and multimodal_examples:\n",
        "        if len(text_only_examples) >= len(multimodal_examples):\n",
        "            examples = text_only_examples\n",
        "        else:\n",
        "            examples = multimodal_examples\n",
        "\n",
        "    batch_type = examples[0][\"data_type\"]\n",
        "\n",
        "    if batch_type == \"text_only\":\n",
        "        texts = []\n",
        "        for example in examples:\n",
        "            text = processor.apply_chat_template(\n",
        "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "            ).strip()\n",
        "            texts.append(text)\n",
        "\n",
        "        batch = processor(text=texts, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    else:\n",
        "        texts = []\n",
        "        images = []\n",
        "        for example in examples:\n",
        "            text = processor.apply_chat_template(\n",
        "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "            ).strip()\n",
        "            texts.append(text)\n",
        "            images.append([example[\"image\"].convert(\"RGB\")])\n",
        "\n",
        "        batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        try:\n",
        "            image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
        "                processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "            )\n",
        "            labels[labels == image_token_id] = -100\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "UKhvJka4kEqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "num_train_epochs = 10\n",
        "learning_rate = 1e-4\n",
        "\n",
        "unified_args = SFTConfig(\n",
        "    output_dir=\"gemma-4b-dengue-diagnosis\",                # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
        "    per_device_train_batch_size=2,                           # Smaller batch size for medical data\n",
        "    per_device_eval_batch_size=2,                            # Smaller batch size for medical data\n",
        "    gradient_accumulation_steps=2,                           # Higher accumulation for effective batch size\n",
        "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
        "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
        "    logging_steps=2,                                         # More frequent logging for small dataset\n",
        "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=2,\n",
        "    learning_rate=learning_rate,                             # Learning rate for medical domain\n",
        "    bf16=True,                                               # Use bfloat16 precision\n",
        "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                                        # Higher warmup for medical domain\n",
        "    lr_scheduler_type=\"cosine\",                              # Cosine scheduler for better convergence\n",
        "    push_to_hub=True,                                        # Push model to Hub\n",
        "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
        "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
        "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
        ")"
      ],
      "metadata": {
        "id": "3IITpXdjkGw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train on Both Text and Image Data\n",
        "\n"
      ],
      "metadata": {
        "id": "5MSf25nTkI2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== MULTIMODAL TRAINING ===\")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "unified_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=unified_args,\n",
        "    train_dataset=unified_data[\"train\"],\n",
        "    eval_dataset=unified_data[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=unified_collate_fn,\n",
        ")\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training on {len(unified_data['train'])} total samples:\")\n",
        "print(f\"  - Text-only: {train_text_count} samples\")\n",
        "print(f\"  - Multimodal: {train_multimodal_count} samples\")"
      ],
      "metadata": {
        "id": "YLqjTI6PkIn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unified_training_result = unified_trainer.train()\n",
        "print(\"Training completed!\")\n",
        "\n",
        "unified_trainer.save_model()\n",
        "print(f\"Model saved to {unified_args.output_dir}\")\n",
        "\n",
        "print(\"\\n=== TRAINING COMPLETED ===\")"
      ],
      "metadata": {
        "id": "vC3pTK2XkMhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the fine-tuned model\n"
      ],
      "metadata": {
        "id": "54JDPsP6piaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del unified_trainer\n",
        "del model"
      ],
      "metadata": {
        "id": "tW5mFE215eGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ZW_phzV8RatO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up for evaluation"
      ],
      "metadata": {
        "id": "zW2o73pBy-FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_detailed_metrics(predictions: list[int], references: list[int]) -> dict[str, float]:\n",
        "    \"\"\"Compute comprehensive evaluation metrics for dengue classification.\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    metrics.update(accuracy_metric.compute(predictions=predictions, references=references))\n",
        "\n",
        "    # Ensure f1, precision, and recall are always computed, even if some classes are missing\n",
        "    try:\n",
        "        metrics.update(f1_metric.compute(predictions=predictions, references=references, average=\"weighted\"))\n",
        "    except ValueError:\n",
        "        metrics[\"f1\"] = 0.0 # Assign 0 if computation fails (e.g., no samples for a class)\n",
        "\n",
        "    try:\n",
        "        metrics.update(precision_metric.compute(predictions=predictions, references=references, average=\"weighted\"))\n",
        "    except ValueError:\n",
        "        metrics[\"precision\"] = 0.0\n",
        "\n",
        "    try:\n",
        "        metrics.update(recall_metric.compute(predictions=predictions, references=references, average=\"weighted\"))\n",
        "    except ValueError:\n",
        "        metrics[\"recall\"] = 0.0\n",
        "\n",
        "\n",
        "    if len(set(references)) == 2:\n",
        "        try:\n",
        "            metrics.update(f1_metric.compute(predictions=predictions, references=references, average=\"binary\"))\n",
        "            metrics[\"f1_binary\"] = metrics.pop(\"f1\") # Rename the binary f1\n",
        "            # Re-compute weighted f1 explicitly after popping the binary one\n",
        "            metrics[\"f1_weighted\"] = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")[\"f1\"]\n",
        "        except ValueError:\n",
        "             metrics[\"f1_binary\"] = 0.0\n",
        "             metrics[\"f1_weighted\"] = metrics.get(\"f1\", 0.0) # Use the possibly already computed weighted or 0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def postprocess_dengue_response(response_text: str) -> int:\n",
        "    \"\"\"Extract dengue diagnosis from model response with improved parsing.\"\"\"\n",
        "    response_text = response_text.lower().strip()\n",
        "\n",
        "    # More comprehensive positive patterns\n",
        "    positive_patterns = [\n",
        "        \"yes - dengue positive\",\n",
        "        \"dengue positive\",\n",
        "        \"yes, dengue positive\",\n",
        "        \"positive for dengue\",\n",
        "        \"dengue fever positive\",\n",
        "        \"has dengue\",\n",
        "        \"dengue diagnosis: positive\",\n",
        "        \"positive\",\n",
        "        \"yes\"\n",
        "    ]\n",
        "\n",
        "    negative_patterns = [\n",
        "        \"no - dengue negative\",\n",
        "        \"dengue negative\",\n",
        "        \"no, dengue negative\",\n",
        "        \"negative for dengue\",\n",
        "        \"dengue fever negative\",\n",
        "        \"no dengue\",\n",
        "        \"dengue diagnosis: negative\",\n",
        "        \"negative\",\n",
        "        \"no\"\n",
        "    ]\n",
        "\n",
        "    # Check positive patterns first (more specific to less specific)\n",
        "    for pattern in positive_patterns:\n",
        "        if pattern in response_text:\n",
        "            return 1\n",
        "\n",
        "    # Then check negative patterns (more specific to less specific)\n",
        "    for pattern in negative_patterns:\n",
        "        if pattern in response_text:\n",
        "            return 0\n",
        "\n",
        "    # Additional heuristics - look for key medical indicators\n",
        "    if any(word in response_text for word in [\"fever\", \"platelet\", \"headache\", \"symptoms\"]):\n",
        "        # If mentions medical terms, try to infer from context\n",
        "        if any(word in response_text for word in [\"consistent\", \"likely\", \"probable\", \"suggests\"]):\n",
        "            return 1\n",
        "        elif any(word in response_text for word in [\"unlikely\", \"not consistent\", \"does not\", \"cannot\"]):\n",
        "            return 0\n",
        "\n",
        "    # Default to unknown if can't parse\n",
        "    print(f\"    Warning: Could not parse response: '{response_text[:200]}...'\")\n",
        "    return -1\n",
        "\n",
        "print(\"Evaluation metrics setup completed.\")"
      ],
      "metadata": {
        "id": "vGNz-jf_Brop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare evaluation data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "eval_df = pd.read_csv(\"/content/drive/My Drive/PDS I/eval/eval_dengue.csv\")\n",
        "eval_df = eval_df.fillna('unknown')\n",
        "\n",
        "eval_df['dengue_binary'] = eval_df['dengue.dengue'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "eval_df = eval_df.dropna(subset=['dengue_binary'])\n",
        "eval_df = eval_df[eval_df['dengue_binary'].notna()]\n",
        "\n",
        "print(f\"\\nEvaluation Dataset:\")\n",
        "print(f\"Total clinical records: {len(eval_df)}\")\n",
        "print(f\"Dengue positive cases: {eval_df['dengue_binary'].sum()}\")\n",
        "print(f\"Dengue negative cases: {(eval_df['dengue_binary'] == 0).sum()}\")\n",
        "\n",
        "\n",
        "image_eval_base_path = \"/content/drive/My Drive/PDS I/eval/images\"\n",
        "eval_image_files = glob.glob(f\"{image_eval_base_path}/*.png\") + glob.glob(f\"{image_eval_base_path}/*.jpg\") + glob.glob(f\"{image_eval_base_path}/*.jpeg\")\n",
        "print(f\"\\nEvaluation images: {len(eval_image_files)}\")\n",
        "\n",
        "eval_dataset_list = []\n",
        "\n",
        "for idx, row in eval_df.iterrows():\n",
        "    try:\n",
        "        formatted_data = format_text_only_data(row)\n",
        "        eval_dataset_list.append(formatted_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing eval row {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "for image_path in eval_image_files:\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        formatted_data = format_multimodal_data(image_path)\n",
        "        formatted_data['image'] = img\n",
        "        eval_dataset_list.append(formatted_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing eval image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nTotal evaluation samples: {len(eval_dataset_list)}\")\n",
        "print(f\"Text-only samples: {len(eval_df)}\")\n",
        "print(f\"Multimodal samples: {len(eval_image_files)}\")\n",
        "\n",
        "text_eval_samples = [sample for sample in eval_dataset_list if sample.get('data_type') == 'text_only']\n",
        "multimodal_eval_samples = [sample for sample in eval_dataset_list if sample.get('data_type') == 'multimodal']\n",
        "\n",
        "print(f\"\\nSeparated for evaluation:\")\n",
        "print(f\"Text-only evaluation samples: {len(text_eval_samples)}\")\n",
        "print(f\"Multimodal evaluation samples: {len(multimodal_eval_samples)}\")"
      ],
      "metadata": {
        "id": "-mC1ETLIBwIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "print(\"=== LOADING FINE-TUNED MODEL ===\")\n",
        "\n",
        "ft_model_path = \"gemma-4b-dengue-diagnosis\"\n",
        "\n",
        "try:\n",
        "    ft_pipe = pipeline(\n",
        "        \"image-text-to-text\",\n",
        "        model=ft_model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    ft_pipe.model.generation_config.do_sample = False\n",
        "    ft_pipe.model.generation_config.pad_token_id = ft_pipe.tokenizer.eos_token_id\n",
        "\n",
        "    ft_pipe.tokenizer.padding_side = \"left\"\n",
        "\n",
        "    print(f\"Successfully loaded fine-tuned model from Hugging Face Hub: {ft_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model from Hub: {e}\")\n",
        "\n",
        "    try:\n",
        "        ft_pipe = pipeline(\n",
        "            \"image-text-to-text\",\n",
        "            model=\"./gemma-4b-dengue-diagnosis\",\n",
        "            processor=processor,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        ft_pipe.model.generation_config.do_sample = False\n",
        "        ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
        "        ft_pipe.tokenizer.padding_side = \"left\"\n",
        "\n",
        "        print(\"Successfully loaded fine-tuned model from local path\")\n",
        "\n",
        "    except Exception as local_error:\n",
        "        print(f\"Error loading from local path: {local_error}\")"
      ],
      "metadata": {
        "id": "nLLO6Hbmy2pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation on both text-only and multimodal samples\n",
        "print(\"=== MODEL EVALUATION ===\")\n",
        "\n",
        "# Initialize evaluation data structures\n",
        "all_predictions = []\n",
        "all_references = []\n",
        "text_predictions = []\n",
        "text_references = []\n",
        "multimodal_predictions = []\n",
        "multimodal_references = []\n",
        "\n",
        "# Check if model loaded successfully\n",
        "try:\n",
        "    # Test the model with a simple medical prompt\n",
        "    test_prompt = \"Based on clinical symptoms of fever and headache, is this dengue? Respond with Yes or No.\"\n",
        "    test_response = ft_pipe(test_prompt, max_new_tokens=50, return_full_text=False)\n",
        "    print(f\"Model test response: {test_response}\")\n",
        "except Exception as e:\n",
        "    print(f\"Model test failed: {e}\")\n",
        "    print(\"The model may not have loaded correctly. Please check the previous cells.\")\n",
        "\n",
        "# Check what special tokens are available\n",
        "print(f\"Available special tokens: {list(ft_pipe.tokenizer.special_tokens_map.keys())}\")\n",
        "\n",
        "# Evaluate text-only samples\n",
        "print(f\"\\nEvaluating {len(text_eval_samples)} text-only samples...\")\n",
        "\n",
        "if len(text_eval_samples) > 0:\n",
        "    # Iterate through all text-only evaluation samples\n",
        "    for i, sample in enumerate(text_eval_samples):\n",
        "        try:\n",
        "            messages = sample[\"messages\"]\n",
        "            ref = sample[\"label\"]\n",
        "\n",
        "            # Use the chat template for text-only samples\n",
        "            try:\n",
        "                chat_text = ft_pipe.tokenizer.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "                input_text = chat_text\n",
        "\n",
        "                # Run inference\n",
        "                output = ft_pipe(\n",
        "                    input_text,\n",
        "                    max_new_tokens=200, # Increased max_new_tokens\n",
        "                    return_full_text=False,\n",
        "                    temperature=0.1,  # Low temperature for more consistent outputs\n",
        "                )\n",
        "\n",
        "                # Extract response\n",
        "                response_text = \"\"\n",
        "                if isinstance(output, list) and len(output) > 0:\n",
        "                    if isinstance(output[0], dict) and \"generated_text\" in output[0]:\n",
        "                        response_text = output[0][\"generated_text\"].strip()\n",
        "                    else:\n",
        "                        # Attempt to convert other list formats\n",
        "                        try:\n",
        "                            response_text = str(output[0]).strip()\n",
        "                        except Exception:\n",
        "                            pass # Keep response_text as empty if conversion fails\n",
        "                else:\n",
        "                    # Attempt to convert other output formats\n",
        "                    try:\n",
        "                        response_text = str(output).strip()\n",
        "                    except Exception:\n",
        "                        pass # Keep response_text as empty if conversion fails\n",
        "\n",
        "\n",
        "                # print(f\"  Sample {i+1} Text Response: '{response_text}'\") # Uncomment for debugging\n",
        "\n",
        "                if response_text and len(response_text) > 0:\n",
        "                    # Try to parse the response\n",
        "                    pred = postprocess_dengue_response(response_text)\n",
        "\n",
        "                    if pred != -1:  # Valid prediction\n",
        "                        text_predictions.append(pred)\n",
        "                        text_references.append(ref)\n",
        "                        # print(f\"  â Text-only prediction: {'Positive' if pred == 1 else 'Negative'} (Expected: {'Positive' if ref == 1 else 'Negative'})\") # Uncomment for debugging\n",
        "                    else:\n",
        "                        print(f\"  Warning: Could not parse text response for sample {i+1}: '{response_text[:100]}...'\")\n",
        "                else:\n",
        "                    print(f\"  Warning: Empty text response for sample {i+1}\")\n",
        "                    print(f\"    Input text: {input_text[:200]}...\") # Add debug print for empty response\n",
        "                    print(f\"    Raw output: {output}\") # Add debug print for raw output\n",
        "\n",
        "\n",
        "            except Exception as text_e:\n",
        "                print(f\"  Error processing text sample {i+1}: {text_e}\")\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in text sample loop {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"â Finished evaluating {len(text_eval_samples)} text-only samples\")\n",
        "\n",
        "# Evaluate multimodal samples using direct model inference\n",
        "print(f\"\\nEvaluating {len(multimodal_eval_samples)} multimodal samples using direct model inference...\")\n",
        "\n",
        "if len(multimodal_eval_samples) > 0:\n",
        "    # Get the model and processor directly from the pipeline\n",
        "    model = ft_pipe.model\n",
        "    processor = ft_pipe.tokenizer # Use the tokenizer from the pipeline\n",
        "\n",
        "    # Set padding side for generation\n",
        "    processor.padding_side = \"left\"\n",
        "\n",
        "    # Try to find the correct image token from available special tokens\n",
        "    image_token = None\n",
        "    for token_name in ['image_token', 'boi_token']: # Check common names\n",
        "        if token_name in processor.special_tokens_map:\n",
        "            image_token = processor.special_tokens_map[token_name]\n",
        "            break\n",
        "\n",
        "    if image_token is None:\n",
        "        print(\"Warning: Could not find a suitable image token in the tokenizer's special tokens map.\")\n",
        "        print(\"Multimodal evaluation may fail.\")\n",
        "        # Fallback to a common placeholder if no special token is found\n",
        "        image_token = \"<image>\"\n",
        "\n",
        "\n",
        "    for i, sample in enumerate(multimodal_eval_samples):\n",
        "        try:\n",
        "            messages = sample[\"messages\"]\n",
        "            image = sample[\"image\"]\n",
        "            ref = sample[\"label\"]\n",
        "\n",
        "            # Manually construct the prompt with the image token placed before the text\n",
        "            prompt_text = \"\"\n",
        "            for content_item in messages[0][\"content\"]:\n",
        "                if content_item[\"type\"] == \"text\":\n",
        "                    prompt_text = content_item[\"text\"]\n",
        "                    break\n",
        "\n",
        "            # Construct the prompt string with the image token explicitly\n",
        "            prompt_with_image_token = f\"{image_token}\\n{prompt_text}\"\n",
        "\n",
        "\n",
        "            try:\n",
        "                # Prepare inputs using the processor with the text containing the image token and the image\n",
        "                inputs = processor(text=prompt_with_image_token, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "                # Move inputs to the same device as the model\n",
        "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Generate response directly from the model\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.1,\n",
        "                    pad_token_id=processor.eos_token_id # Use eos_token_id for padding\n",
        "                )\n",
        "\n",
        "                # Decode the generated tokens\n",
        "                # Need to decode from the point after the input prompt\n",
        "                # Find the length of the input prompt\n",
        "                input_length = inputs[\"input_ids\"].shape[1]\n",
        "                generated_text = processor.decode(output_ids[0, input_length:], skip_special_tokens=True)\n",
        "                response_text = generated_text.strip()\n",
        "\n",
        "                # print(f\"  Sample {i+1} Multimodal Response: '{response_text}'\") # Uncomment for debugging\n",
        "\n",
        "                if response_text and len(response_text) > 0:\n",
        "                    pred = postprocess_dengue_response(response_text)\n",
        "                    if pred != -1:\n",
        "                        multimodal_predictions.append(pred)\n",
        "                        multimodal_references.append(ref)\n",
        "                        # print(f\"  â Multimodal prediction: {'Positive' if pred == 1 else 'Negative'} (Expected: {'Positive' if ref == 1 else 'Negative'})\") # Uncomment for debugging\n",
        "                    else:\n",
        "                        print(f\"  Warning: Could not parse multimodal response for sample {i+1}: '{response_text[:100]}...'\")\n",
        "                else:\n",
        "                    print(f\"  Warning: Empty multimodal response for sample {i+1}\")\n",
        "\n",
        "\n",
        "            except Exception as multimodal_e:\n",
        "                 print(f\"  Error during multimodal inference for sample {i+1}: {multimodal_e}\")\n",
        "                 continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in multimodal sample loop {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"â Finished evaluating {len(multimodal_eval_samples)} multimodal samples\")\n",
        "\n",
        "\n",
        "# Combine all predictions for overall metrics\n",
        "all_predictions = text_predictions + multimodal_predictions\n",
        "all_references = text_references + multimodal_references\n",
        "\n",
        "print(f\"\\n=== EVALUATION SUMMARY ===\")\n",
        "print(f\"Total evaluation samples attempted: {len(text_eval_samples) + len(multimodal_eval_samples)}\")\n",
        "print(f\"Total valid predictions generated: {len(all_predictions)}\")\n",
        "print(f\"Text-only predictions: {len(text_predictions)}\")\n",
        "print(f\"Multimodal predictions: {len(multimodal_predictions)}\")\n",
        "\n",
        "# Now, compute and display the detailed metrics using the collected predictions and references\n",
        "print(\"\\nComputing detailed metrics...\")\n",
        "\n",
        "if len(all_predictions) > 0:\n",
        "    # Overall unified model performance\n",
        "    overall_metrics = compute_detailed_metrics(all_predictions, all_references)\n",
        "    print(f\"\\nOVERALL UNIFIED MODEL PERFORMANCE:\")\n",
        "    print(f\"   Accuracy: {overall_metrics['accuracy']:.3f}\")\n",
        "    print(f\"   F1-Score (Weighted): {overall_metrics['f1']:.3f}\")\n",
        "    print(f\"   Precision (Weighted): {overall_metrics['precision']:.3f}\")\n",
        "    print(f\"   Recall (Weighted): {overall_metrics['recall']:.3f}\")\n",
        "\n",
        "    if 'f1_binary' in overall_metrics:\n",
        "        print(f\"   F1-Score (Binary): {overall_metrics['f1_binary']:.3f}\")\n",
        "    if 'f1_weighted' in overall_metrics:\n",
        "        print(f\"   F1-Score (Weighted - check): {overall_metrics['f1_weighted']:.3f}\")\n",
        "\n",
        "\n",
        "    # Text-only performance\n",
        "    if len(text_predictions) > 0:\n",
        "        text_metrics = compute_detailed_metrics(text_predictions, text_references)\n",
        "        print(f\"\\nð TEXT-ONLY PERFORMANCE:\")\n",
        "        print(f\"   Samples: {len(text_predictions)}\")\n",
        "        print(f\"   Accuracy: {text_metrics['accuracy']:.3f}\")\n",
        "        print(f\"   F1-Score: {text_metrics['f1']:.3f}\")\n",
        "        print(f\"   Precision: {text_metrics['precision']:.3f}\")\n",
        "        print(f\"   Recall: {text_metrics['recall']:.3f}\")\n",
        "\n",
        "    # Multimodal performance\n",
        "    if len(multimodal_predictions) > 0:\n",
        "        multimodal_metrics = compute_detailed_metrics(multimodal_predictions, multimodal_references)\n",
        "        print(f\"\\nMULTIMODAL PERFORMANCE:\")\n",
        "        print(f\"   Samples: {len(multimodal_predictions)}\")\n",
        "        print(f\"   Accuracy: {multimodal_metrics['accuracy']:.3f}\")\n",
        "        print(f\"   F1-Score: {multimodal_metrics['f1']:.3f}\")\n",
        "        print(f\"   Precision: {multimodal_metrics['precision']:.3f}\")\n",
        "        print(f\"   Recall: {multimodal_metrics['recall']:.3f}\")\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(f\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
        "    try:\n",
        "        print(classification_report(\n",
        "            all_references,\n",
        "            all_predictions,\n",
        "            target_names=[\"No Dengue\", \"Dengue Positive\"],\n",
        "            digits=3\n",
        "        ))\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not generate classification report: {e}\")\n",
        "         print(\"This might happen if there are no samples for one of the classes in the predictions/references.\")\n",
        "\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(f\"\\nð CONFUSION MATRIX:\")\n",
        "    try:\n",
        "        cm = confusion_matrix(all_references, all_predictions)\n",
        "        print(\"              Predicted\")\n",
        "        print(\"              No    Yes\")\n",
        "        print(f\"Actual No  : {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "        print(f\"Actual Yes : {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not generate confusion matrix: {e}\")\n",
        "        print(\"This might happen if there are no samples for one of the classes in the predictions/references.\")\n",
        "\n",
        "\n",
        "    # Performance breakdown by data type\n",
        "    if len(text_predictions) > 0 and len(multimodal_predictions) > 0:\n",
        "        print(f\"\\nPERFORMANCE COMPARISON:\")\n",
        "        print(f\"Text-only Accuracy:   {text_metrics['accuracy']:.3f}\")\n",
        "        print(f\"Multimodal Accuracy:  {multimodal_metrics['accuracy']:.3f}\")\n",
        "        print(f\"Improvement:          {multimodal_metrics['accuracy'] - text_metrics['accuracy']:+.3f}\")\n",
        "\n",
        "        print(f\"\\nText-only F1:         {text_metrics['f1']:.3f}\")\n",
        "        print(f\"Multimodal F1:        {multimodal_metrics['f1']:.3f}\")\n",
        "        print(f\"F1 Improvement:       {multimodal_metrics['f1'] - text_metrics['f1']:+.3f}\")\n",
        "\n",
        "else:\n",
        "    print(\"â ï¸ No valid predictions were generated for evaluation.\")\n",
        "    print(\"Please check the model loading and inference configuration.\")\n",
        "\n",
        "print(\"\\n=== EVALUATION COMPLETED ===\")"
      ],
      "metadata": {
        "id": "FEfJ83FbLYcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question = \"Patient from New Delhi. fever duration: 4 days. current temperature: 104Â°F. Lab values - WBC: 1.0, Hemoglobin: 9.0, Platelet: 80.0. Symptoms: joint_muscle_aches, pain_behind_the_eyes. Does Patient has dengue fever?\"\n",
        "generator = pipeline(\"text-generation\", model=\"google/gemma-3-4b-it\", device=\"cuda\")\n",
        "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\n",
        "print(output[\"generated_text\"])"
      ],
      "metadata": {
        "id": "F6bwMh7iR87p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"Patient from Bangalore. fever duration: 10 days. current temperature: 100.0Â°F. Lab values - WBC: 5.0, Hemoglobin: 15.0, Platelet: 140.0. Symptoms: severe headache, metallic taste, appetite loss, abdominal pain, diarrhea. Does Patient has dengue fever?\"\n",
        "question = \"Patient from New Delhi. fever duration: 4 days. current temperature: 104Â°F. Lab values - WBC: 1.0, Hemoglobin: 9.0, Platelet: 80.0. Symptoms: joint_muscle_aches, pain_behind_the_eyes. Does Patient has dengue fever?\"\n",
        "generator = pipeline(\"text-generation\", model=\"longbao128/gemma-4b-dengue-diagnosis\", device=\"cuda\")\n",
        "output = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\n",
        "print(output[\"generated_text\"])"
      ],
      "metadata": {
        "id": "FBSeKlqkSI8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del generator\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ZRnnxzA2Se4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display detailed evaluation metrics\n",
        "print(\"=== DETAILED EVALUATION RESULTS ===\")\n",
        "\n",
        "if len(all_predictions) > 0:\n",
        "    # Overall unified model performance\n",
        "    overall_metrics = compute_detailed_metrics(all_predictions, all_references)\n",
        "    print(f\"\\nOVERALL UNIFIED MODEL PERFORMANCE:\")\n",
        "    print(f\"   Accuracy: {overall_metrics['accuracy']:.3f}\")\n",
        "    print(f\"   F1-Score (Weighted): {overall_metrics['f1']:.3f}\")\n",
        "    print(f\"   Precision (Weighted): {overall_metrics['precision']:.3f}\")\n",
        "    print(f\"   Recall (Weighted): {overall_metrics['recall']:.3f}\")\n",
        "\n",
        "    if 'f1_binary' in overall_metrics:\n",
        "        print(f\"   F1-Score (Binary): {overall_metrics['f1_binary']:.3f}\")\n",
        "\n",
        "    # Text-only performance\n",
        "    if len(text_predictions) > 0:\n",
        "        text_metrics = compute_detailed_metrics(text_predictions, text_references)\n",
        "        print(f\"\\nð TEXT-ONLY PERFORMANCE:\")\n",
        "        print(f\"   Samples: {len(text_predictions)}\")\n",
        "        print(f\"   Accuracy: {text_metrics['accuracy']:.3f}\")\n",
        "        print(f\"   F1-Score: {text_metrics['f1']:.3f}\")\n",
        "        print(f\"   Precision: {text_metrics['precision']:.3f}\")\n",
        "        print(f\"   Recall: {text_metrics['recall']:.3f}\")\n",
        "\n",
        "    # Multimodal performance\n",
        "    if len(multimodal_predictions) > 0:\n",
        "        multimodal_metrics = compute_detailed_metrics(multimodal_predictions, multimodal_references)\n",
        "        print(f\"\\nMULTIMODAL PERFORMANCE:\")\n",
        "        print(f\"   Samples: {len(multimodal_predictions)}\")\n",
        "        print(f\"   Accuracy: {multimodal_metrics['accuracy']:.3f}\")\n",
        "        print(f\"   F1-Score: {multimodal_metrics['f1']:.3f}\")\n",
        "        print(f\"   Precision: {multimodal_metrics['precision']:.3f}\")\n",
        "        print(f\"   Recall: {multimodal_metrics['recall']:.3f}\")\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(f\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
        "    print(classification_report(\n",
        "        all_references,\n",
        "        all_predictions,\n",
        "        target_names=[\"No Dengue\", \"Dengue Positive\"],\n",
        "        digits=3\n",
        "    ))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(f\"\\nð CONFUSION MATRIX:\")\n",
        "    cm = confusion_matrix(all_references, all_predictions)\n",
        "    print(\"              Predicted\")\n",
        "    print(\"              No    Yes\")\n",
        "    print(f\"Actual No  : {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "    print(f\"Actual Yes : {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "\n",
        "    # Performance breakdown by data type\n",
        "    if len(text_predictions) > 0 and len(multimodal_predictions) > 0:\n",
        "        print(f\"\\nPERFORMANCE COMPARISON:\")\n",
        "        print(f\"Text-only Accuracy:   {text_metrics['accuracy']:.3f}\")\n",
        "        print(f\"Multimodal Accuracy:  {multimodal_metrics['accuracy']:.3f}\")\n",
        "        print(f\"Improvement:          {multimodal_metrics['accuracy'] - text_metrics['accuracy']:+.3f}\")\n",
        "\n",
        "        print(f\"\\nText-only F1:         {text_metrics['f1']:.3f}\")\n",
        "        print(f\"Multimodal F1:        {multimodal_metrics['f1']:.3f}\")\n",
        "        print(f\"F1 Improvement:       {multimodal_metrics['f1'] - text_metrics['f1']:+.3f}\")\n",
        "\n",
        "else:\n",
        "    print(\"â ï¸ No valid predictions were generated for evaluation.\")\n",
        "    print(\"Please check the model loading and inference configuration.\")\n",
        "\n",
        "print(\"\\n=== EVALUATION COMPLETED ===\")"
      ],
      "metadata": {
        "id": "J4Vu6onsB11z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2DXu3GXX2vFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
